{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsijm/OneDrive - Bol.com/Other/prive/NLP/Gensim/GensimTutorial/venv/lib/python3.7/site-packages/ipykernel_launcher.py:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/jsijm/OneDrive - Bol.com/Other/prive/NLP/Gensim/GensimTutorial/venv/lib/python3.7/site-packages/ipykernel_launcher.py:20: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download dataset\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# Split the data into 2 parts. Part 2 will be used later to update the model\n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Get the word vector for given word\n",
    "model['topic']\n",
    "#> array([ 0.0512,  0.2555,  0.9393, ... ,-0.5669,  0.6737], dtype=float32)\n",
    "\n",
    "model.wv.most_similar('topic')\n",
    "#> [('discussion', 0.7590423822402954),\n",
    "#>  ('consensus', 0.7253159284591675),\n",
    "#>  ('discussions', 0.7252693176269531),\n",
    "#>  ('interpretation', 0.7196053266525269),\n",
    "#>  ('viewpoint', 0.7053568959236145),\n",
    "#>  ('speculation', 0.7021505832672119),\n",
    "#>  ('discourse', 0.7001898884773254),\n",
    "#>  ('opinions', 0.6993060111999512),\n",
    "#>  ('focus', 0.6959210634231567),\n",
    "#>  ('scholarly', 0.6884037256240845)]\n",
    "\n",
    "# Save and Load Model\n",
    "model.save('newmodel')\n",
    "model = Word2Vec.load('newmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained and saved a Word2Vec model for our document. However, when a new dataset comes, you want to update the model so as to account for new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsijm/OneDrive - Bol.com/Other/prive/NLP/Gensim/GensimTutorial/venv/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/jsijm/OneDrive - Bol.com/Other/prive/NLP/Gensim/GensimTutorial/venv/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.22637856e+00,  1.64273947e-01,  1.68680465e+00, -1.25953648e-02,\n",
       "        1.32304585e+00,  6.36845887e-01,  1.26943934e+00, -3.14511001e-01,\n",
       "        3.82891089e-01, -8.78210187e-01,  6.61116600e-01,  1.64003468e+00,\n",
       "       -5.38034618e-01, -1.13696325e+00,  5.80287576e-01,  2.14783072e+00,\n",
       "        2.96406895e-01,  5.31061828e-01, -2.16134548e-01, -5.19048035e-01,\n",
       "       -1.93767071e-01, -5.28440952e-01, -4.06028777e-02, -1.18609929e+00,\n",
       "        1.07600462e+00, -1.91383019e-01, -1.28334895e-01,  8.73369515e-01,\n",
       "        5.59210293e-02,  1.33039963e+00,  7.11525744e-03,  3.79143238e-01,\n",
       "       -1.22011542e+00, -1.01336217e+00,  9.41247344e-02, -8.72300506e-01,\n",
       "        8.39870647e-02,  1.70967773e-01, -6.06927991e-01,  1.23837292e+00,\n",
       "        1.95503819e+00, -9.15693343e-02,  1.22758114e+00, -2.14937091e+00,\n",
       "        3.81040648e-02,  8.46357822e-01,  7.06874505e-02,  2.33334884e-01,\n",
       "        1.04944420e+00,  9.67311978e-01,  9.15687323e-01, -9.23784554e-01,\n",
       "        1.44439912e+00, -3.88119191e-01, -1.12741780e+00, -3.22417438e-01,\n",
       "       -2.64203697e-01, -1.79083312e+00, -6.05844438e-01, -6.97502255e-01,\n",
       "        6.73484206e-01,  1.73437133e-01, -1.09525740e+00, -8.26904595e-01,\n",
       "        6.55085087e-01,  3.33336860e-01,  1.06951475e-01, -6.91660106e-01,\n",
       "        3.44315602e-04, -8.72156084e-01, -1.96553543e-01,  1.29952502e+00,\n",
       "       -6.40882373e-01, -7.96156883e-01, -1.00706160e+00,  4.16165262e-01,\n",
       "        8.10171604e-01, -6.07144296e-01, -2.02204180e+00,  2.29345903e-01,\n",
       "        5.13347030e-01, -9.88921702e-01, -1.09491849e+00, -8.69197071e-01,\n",
       "        1.13563808e-02,  2.47533023e-01, -4.39130008e-01,  3.94316524e-01,\n",
       "        4.26064700e-01,  1.23411453e+00, -6.01541996e-02, -1.92798525e-01,\n",
       "       -9.45016146e-01,  1.34823239e+00,  1.30889130e+00,  6.48352206e-01,\n",
       "        1.88603032e+00,  3.86039913e-01,  1.99426877e+00,  2.23979807e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the model with new data.\n",
    "model.build_vocab(data_part2, update=True)\n",
    "model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model['topic']\n",
    "# array([-0.6482, -0.5468,  1.0688,  0.82  , ... , -0.8411,  0.3974], dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. How to extract word vectors using pre-trained Word2Vec and FastText models?\n",
    "We just saw how to get the word vectors for Word2Vec model we just trained. However, gensim lets you download state of the art pretrained models through the downloader API. Letâ€™s see how to extract the word vectors from a couple of these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=======================================-----------] 79.8% 764.9/958.4MB downloaded"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# Download the models\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "glove_model300 = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "# Get word embeddings\n",
    "word2vec_model300.most_similar('support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2ec_accuracy\n",
    "word2vec_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "#> 0.7401448525607863\n",
    "\n",
    "# fasttext_accuracy\n",
    "fasttext_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "#> 0.8827876424099353\n",
    "\n",
    "# GloVe accuracy\n",
    "glove_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "#> 0.7195422354510931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GensimTut_Python3.7",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
