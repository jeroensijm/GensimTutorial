{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. How to compute similarity metrics like cosine similarity and soft cosine similarity?\n",
    "Soft cosine similarity is similar to cosine similarity but in addition considers the semantic relationship between the words through its vector representation.\n",
    "\n",
    "To compute soft cosines, you will need a word embedding model like Word2Vec or FastText. First, compute the similarity_matrix. Then convert the input sentences to bag-of-words corpus and pass them to the softcossim() along with the similarity matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-74e9bb868be7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Prepare the similarity matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0msimilarity_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext_model300\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnonzero_limit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Prepare a dictionary and a corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from gensim.matutils import softcossim\n",
    "from gensim import corpora\n",
    "import gensim.downloader as api\n",
    "\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "sent_1 = 'Sachin is a cricket player and a opening batsman'.split()\n",
    "sent_2 = 'Dhoni is a cricket player too He is a batsman and keeper'.split()\n",
    "sent_3 = 'Anand is a chess player'.split()\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "# Prepare a dictionary and a corpus.\n",
    "documents = [sent_1, sent_2, sent_3]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Convert the sentences into bag-of-words vectors.\n",
    "sent_1 = dictionary.doc2bow(sent_1)\n",
    "sent_2 = dictionary.doc2bow(sent_2)\n",
    "sent_3 = dictionary.doc2bow(sent_3)\n",
    "\n",
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "#> 0.7868705819999783\n",
    "\n",
    "print(softcossim(sent_1, sent_3, similarity_matrix))\n",
    "#> 0.6036445529268666\n",
    "\n",
    "print(softcossim(sent_2, sent_3, similarity_matrix))\n",
    "#> 0.60965453519611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which word from the given list doesn't go with the others?\n",
    "print(fasttext_model300.doesnt_match(['india', 'australia', 'pakistan', 'china', 'beetroot']))  \n",
    "#> beetroot\n",
    "\n",
    "# Compute cosine distance between two words.\n",
    "print(fasttext_model300.distance('king', 'queen'))\n",
    "#> 0.22957539558410645\n",
    "\n",
    "\n",
    "# Compute cosine distances from given word or vector to all words in `other_words`.\n",
    "print(fasttext_model300.distances('king', ['queen', 'man', 'woman']))\n",
    "#> [0.22957546 0.465837   0.547001  ]\n",
    "\n",
    "\n",
    "# Compute cosine similarities\n",
    "print(fasttext_model300.cosine_similarities(fasttext_model300['king'], \n",
    "                                            vectors_all=(fasttext_model300['queen'], \n",
    "                                                        fasttext_model300['man'], \n",
    "                                                        fasttext_model300['woman'],\n",
    "                                                        fasttext_model300['queen'] + fasttext_model300['man'])))  \n",
    "#> array([0.77042454, 0.534163  , 0.45299897, 0.76572555], dtype=float32)\n",
    "# Note: Queen + Man is very similar to King.\n",
    "\n",
    "# Get the words closer to w1 than w2\n",
    "print(glove_model300.words_closer_than(w1='king', w2='kingdom'))\n",
    "#> ['prince', 'queen', 'monarch']\n",
    "\n",
    "\n",
    "# Find the top-N most similar words.\n",
    "print(fasttext_model300.most_similar(positive='king', negative=None, topn=5, restrict_vocab=None, indexer=None))\n",
    "#> [('queen', 0.63), ('prince', 0.62), ('monarch', 0.59), ('kingdom', 0.58), ('throne', 0.56)]\n",
    "\n",
    "\n",
    "# Find the top-N most similar words, using the multiplicative combination objective,\n",
    "print(glove_model300.most_similar_cosmul(positive='king', negative=None, topn=5))\n",
    "#> [('queen', 0.82), ('prince', 0.81), ('monarch', 0.79), ('kingdom', 0.79), ('throne', 0.78)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GensimTut_Python3.7",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
