{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/nlp/gensim-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. How to create a bag of words corpus in gensim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next important object you need to familiarize with in order to work in gensim is the Corpus (a Bag of Words). That is, it is a corpus object that contains the word id and its frequency in each document. You can think of it as gensim’s equivalent of a Document-Term matrix.\n",
    "\n",
    "Once you have the updated dictionary, all you need to do to create a bag of words corpus is to pass the tokenized list of words to the Dictionary.doc2bow()\n",
    "\n",
    "Let’s create s Corpus for a simple list (my_docs) containing 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dogs': 0, 'let': 1, 'out': 2, 'the': 3, 'who': 4}\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "\n",
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "# tokenized_list2 = [[text for text in doc.split()] for doc in my_docs]  simple_preprocess is preferred as it lowercases\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "\n",
    "pprint(mydict.token2id)  #print token to id  \n",
    "pprint(mycorpus)    #print BoW\n",
    "#> [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('dogs', 1), ('let', 1), ('out', 1), ('the', 1), ('who', 1)], [('who', 4)]]\n"
     ]
    }
   ],
   "source": [
    "#or more efficient\n",
    "word_counts = [[(mydict[id], count) for id, count in line] for line in mycorpus]\n",
    "#get the id and count from every line in mycorpus, then fill in that id in mydict to get the token and print the same count\n",
    "pprint(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice, the order of the words gets lost. Just the word and it’s frequency information is retained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. How to create a bag of words corpus from a text file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)]\n",
      "[(14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1)]\n",
      "[(5, 2), (12, 1), (22, 2), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]\n",
      "[(3, 1), (9, 1), (12, 2), (18, 1), (22, 1), (26, 1), (32, 1), (33, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)]\n",
      "[(15, 1), (17, 1), (18, 1), (21, 1)]\n",
      "[(3, 1), (9, 1), (14, 1), (16, 1), (19, 1), (22, 2), (26, 2), (32, 1), (33, 1), (34, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1)]\n",
      "[(3, 1), (5, 2), (9, 1), (10, 1), (12, 1), (13, 1), (18, 1), (43, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1)]\n",
      "[(12, 1), (22, 1), (33, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1)]\n",
      "[(12, 3), (16, 1), (26, 1), (41, 1), (43, 1), (47, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1)]\n",
      "[(12, 1), (15, 1), (18, 1), (57, 1), (70, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 1), (83, 1), (84, 1), (85, 1)]\n",
      "[(5, 1), (12, 2), (22, 1), (32, 1), (33, 1), (42, 1), (86, 1), (87, 1), (88, 1), (89, 1), (90, 1), (91, 1), (92, 1), (93, 1), (94, 1)]\n",
      "[[('army', 1), ('chinaa', 1), ('chinese', 1), ('force', 1), ('liberation', 1), ('of', 1), ('peoplea', 1), ('recently', 1), ('recruited', 1), ('rocket', 1), ('tank', 1), ('technicians', 1), ('the', 1), ('think', 1)], [('companies', 1), ('daily', 1), ('from', 1), ('on', 1), ('pla', 1), ('private', 1), ('reported', 1), ('saturday', 1)], [('of', 2), ('the', 1), ('and', 2), ('appointment', 1), ('at', 1), ('ceremony', 1), ('experts', 1), ('founding', 1), ('hao', 1), ('letters', 1), ('other', 1), ('received', 1), ('science', 1), ('technology', 1), ('zhang', 1)], [('force', 1), ('rocket', 1), ('the', 2), ('pla', 1), ('and', 1), ('experts', 1), ('science', 1), ('technology', 1), ('according', 1), ('by', 1), ('defense', 1), ('national', 1), ('panel', 1), ('published', 1), ('report', 1), ('to', 1)], [('daily', 1), ('on', 1), ('pla', 1), ('saturday', 1)], [('force', 1), ('rocket', 1), ('companies', 1), ('from', 1), ('private', 1), ('and', 2), ('experts', 2), ('science', 1), ('technology', 1), ('zhang', 1), ('as', 1), ('fellow', 1), ('his', 1), ('honored', 1), ('will', 1)], [('force', 1), ('of', 2), ('rocket', 1), ('tank', 1), ('the', 1), ('think', 1), ('pla', 1), ('as', 1), ('will', 1), ('conduct', 1), ('design', 1), ('fields', 1), ('into', 1), ('like', 1), ('members', 1), ('overall', 1), ('research', 1), ('serve', 1), ('which', 1)], [('the', 1), ('and', 1), ('technology', 1), ('five', 1), ('for', 1), ('launching', 1), ('missile', 1), ('missiles', 1), ('network', 1), ('system', 1), ('years', 1)], [('the', 3), ('from', 1), ('experts', 1), ('report', 1), ('as', 1), ('will', 1), ('counterparts', 1), ('enjoy', 1), ('firms', 1), ('owned', 1), ('said', 1), ('same', 1), ('state', 1), ('their', 1), ('treatment', 1)], [('the', 1), ('daily', 1), ('pla', 1), ('which', 1), ('said', 1), ('china', 1), ('civilian', 1), ('deepening', 1), ('development', 1), ('in', 2), ('integration', 1), ('marks', 1), ('military', 1), ('new', 1), ('that', 1), ('this', 1)], [('of', 1), ('the', 2), ('and', 1), ('science', 1), ('technology', 1), ('to', 1), ('better', 1), ('capabilities', 1), ('combat', 1), ('contribute', 1), ('could', 1), ('enhancement', 1), ('forcea', 1), ('innovation', 1), ('make', 1)]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import nltk\n",
    "\n",
    "\n",
    "class BoWCorpus(object):\n",
    "    def __init__(self, path, dictionary):\n",
    "        self.filepath = path\n",
    "        self.dictionary = dictionary\n",
    "\n",
    "    def __iter__(self):\n",
    "        global mydict  # OPTIONAL, only if updating the source dictionary.\n",
    "        for line in smart_open(self.filepath, encoding='latin'):\n",
    "            # tokenize\n",
    "            tokenized_list = simple_preprocess(line, deacc=True)\n",
    "\n",
    "            # create bag of words\n",
    "            bow = self.dictionary.doc2bow(tokenized_list, allow_update=True)\n",
    "\n",
    "            # update the source dictionary (OPTIONAL)\n",
    "            mydict.merge_with(self.dictionary)\n",
    "\n",
    "            # lazy return the BoW\n",
    "            yield bow\n",
    "\n",
    "\n",
    "# Create the Dictionary\n",
    "mydict = corpora.Dictionary()\n",
    "\n",
    "# Create the Corpus\n",
    "bow_corpus = BoWCorpus('sample.txt', dictionary=mydict)  # memory friendly\n",
    "\n",
    "# Print the token_id and count for each line.\n",
    "for line in bow_corpus:\n",
    "    print(line)\n",
    "\n",
    "print([[(mydict[id], count) for id, count in line] for line in bow_corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. How to save a gensim dictionary and corpus to disk and load them back? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Dict and Corpus\n",
    "mydict.save('mydict.dict')  # save dict to disk\n",
    "corpora.MmCorpus.serialize('bow_corpus.mm', bow_corpus)  # save corpus to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (1, 1.0), (2, 1.0), (3, 1.0), (4, 1.0), (5, 1.0), (6, 1.0), (7, 1.0), (8, 1.0), (9, 1.0), (10, 1.0), (11, 1.0), (12, 1.0), (13, 1.0)]\n",
      "[(14, 1.0), (15, 1.0), (16, 1.0), (17, 1.0), (18, 1.0), (19, 1.0), (20, 1.0), (21, 1.0)]\n",
      "[(5, 2.0), (12, 1.0), (22, 2.0), (23, 1.0), (24, 1.0), (25, 1.0), (26, 1.0), (27, 1.0), (28, 1.0), (29, 1.0), (30, 1.0), (31, 1.0), (32, 1.0), (33, 1.0), (34, 1.0)]\n",
      "[(3, 1.0), (9, 1.0), (12, 2.0), (18, 1.0), (22, 1.0), (26, 1.0), (32, 1.0), (33, 1.0), (35, 1.0), (36, 1.0), (37, 1.0), (38, 1.0), (39, 1.0), (40, 1.0), (41, 1.0), (42, 1.0)]\n",
      "[(15, 1.0), (17, 1.0), (18, 1.0), (21, 1.0)]\n",
      "[(3, 1.0), (9, 1.0), (14, 1.0), (16, 1.0), (19, 1.0), (22, 2.0), (26, 2.0), (32, 1.0), (33, 1.0), (34, 1.0), (43, 1.0), (44, 1.0), (45, 1.0), (46, 1.0), (47, 1.0)]\n",
      "[(3, 1.0), (5, 2.0), (9, 1.0), (10, 1.0), (12, 1.0), (13, 1.0), (18, 1.0), (43, 1.0), (47, 1.0), (48, 1.0), (49, 1.0), (50, 1.0), (51, 1.0), (52, 1.0), (53, 1.0), (54, 1.0), (55, 1.0), (56, 1.0), (57, 1.0)]\n",
      "[(12, 1.0), (22, 1.0), (33, 1.0), (58, 1.0), (59, 1.0), (60, 1.0), (61, 1.0), (62, 1.0), (63, 1.0), (64, 1.0), (65, 1.0)]\n",
      "[(12, 3.0), (16, 1.0), (26, 1.0), (41, 1.0), (43, 1.0), (47, 1.0), (66, 1.0), (67, 1.0), (68, 1.0), (69, 1.0), (70, 1.0), (71, 1.0), (72, 1.0), (73, 1.0), (74, 1.0)]\n",
      "[(12, 1.0), (15, 1.0), (18, 1.0), (57, 1.0), (70, 1.0), (75, 1.0), (76, 1.0), (77, 1.0), (78, 1.0), (79, 2.0), (80, 1.0), (81, 1.0), (82, 1.0), (83, 1.0), (84, 1.0), (85, 1.0)]\n",
      "[(5, 1.0), (12, 2.0), (22, 1.0), (32, 1.0), (33, 1.0), (42, 1.0), (86, 1.0), (87, 1.0), (88, 1.0), (89, 1.0), (90, 1.0), (91, 1.0), (92, 1.0), (93, 1.0), (94, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "# Load them back\n",
    "loaded_dict = corpora.Dictionary.load('mydict.dict')\n",
    "\n",
    "corpus = corpora.MmCorpus('bow_corpus.mm')\n",
    "for line in corpus:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GensimTut_Python3.7",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
